---
title: "AirCalibrate: 2nd Meeting"
author: "Xiang Chen"
date: "12/10/2020"
output: 
    html_document:
        toc: true
        toc_float: true
        code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, message = F, fig.width = 12.9, fig.height = 9.6)
```

## 1. Literature review

### 1.1 Stucture of *Introduction* part

**[Health effects of PM2.5]** Air pollution is one of the top threatens in global disease burden and Disability Adjusted Life Year (DALY) [State of Global Air report 2019, Global Disease Burden 2019]. PM2.5, as one of the main components of the air pollution, is particularly concerned by researcher and public. Since Harvard Six Cites research [Dockery 1993], there are lots of papers investigating the relationship between PM2.5 exposure and human health. PM2.5 may cause asthma[Keet 2018], Inflammation in type 1 diabetes[Puett 2019], Lung function and emphysema[Wang 2019], Low birthweight[Fong 2019], Respiratory Viral Infection[Ciencewicki 2007], etc.

**[US air monitoring system]** Due to the harm of PM2.5 and other air pollutants, US government passed the Clean Air Act of 1963. This law authorizes EPA to establish National Ambient Air Quality Standards (NAAQS) to protect public health and public welfare and to regulate emissions of hazardous air pollutants. EPA directing the states to develop state implementation plans and set air pollution monitors. Monitors have FRM and FEM method. Summary the number and distribution of monitors.

**[Disadvantages of AQS and need for low cost sensors]** However, regulatory monitors are expensive, need well-trained scientists to maintain, dense is low, can't reflect spatial variability within small areas. In recent years, the development and application of low cost sensors provide a possible way to solve this problem. Advantages of sensors: low cost, easy to maintain.

**[Disadvantages of low cost sensors and need for calibration]** Neverthless, there are also disadvantages of low cost sensors: accuracy (overestimate), stability (drift, prone to temp, RH, missing data). 

**[Calibration method]**
In general, there are two ways to do calibration, lab calibration and in-field calibration. Lab calibration is accurate, but they are not performed in real world, and is difficult to scale. The other way is in-field calibration. This method often used for mobile sampling, short-term champigns, and long-term automatic online monitoring. There are many statistical methods to do this:

* Simple correction factors[Lim 2019]
* Linear: Gain-offset[Datta 2020, Devito 2018], geographically weighted regression[Bi 2020], linear and orthogonal regression[Johnson 2018a], ridge regression[Johnson 2018b]
* Non-linear: random forests[Zimmerman 2018], neural networks[Topalovic 2019, Devito 2018], gradient boosting regression trees[Johnson 2018b], support vector regression[Devito 2018], Gaussian Processes Regressors[Devito 2018, Monroy2012], Reservoir Computing[Devito 2018], high-dimensional model representation [Cross 2017, Fonollosa 2015], k nearest neighbors[Cross 2017], semi-supervised learning[Martinelli 2013]

**[Why do this research]** Although there are many research, most of them are within city or state level, and time span are less than one year. 

### 1.2 Summarized table for methods paper

```{r}
if(!require("pacman", quietly = T)){
        install.packages(x)
        require(x,character.only = T)
}
pacman::p_load(excelR,magrittr,broom,skimr,naniar,lubridate,knitr,kableExtra,here,tidyverse,readxl)

review <- read_xlsx(here("products","review.xlsx"))

excelTable(review)
```

### 1.3 Questions

- What should I focus on when I read papers?
- Papers about calibrating gas pollutants?
- Papers about lab calibration?
- Papers focusing on other countries? 
- Lim 2019 is not using stacked ensemble method for calibration, but for Land Use Regression.


## 2. GitHub website

Link: https://github.com/wilson-xiangchen/AirCalibrate

- This report and literature review excel file are under ./products.
- Is it safe to upload all the code/reports/drafts to GitHUb?
- Should I upload data file? PurpleAir raw data is huge, around 10M per file, 40M per site, 2GB per state.

## 3. PurpleAir data

There are many questions about the dataset.

__1. Too slow to download__

- If we use 2019.1.1-2020.11.1, 10 min interval. It takes 5min per file, 20min per site, 38h for a state.
- I used an R package for purpleair to download, same speed.
- I applied a PurpleAir API key, haven't tried.

__2. Data is huge__

- 2GB per state for raw data. 1.5GB for tidy data.
- Storage can be a problem
- May run slow when fitting model on my computer. Machine learning skills may not be able to run for whole US.

__3. Should we change research area and time period?__

- Areas: Select some representative states?
- Time span: one year? 60min interval?


## 4. Data processing

Split raw data into 3 lists: covariate (name, lontitude, latitude), channel A, channel B (PM2.5, TEMP, RH).

```{r}

## Read in csv files. Only include Primary data for the sensor.
csv_files <- list.files(here::here("data","raw","MD"), recursive = TRUE,
                          pattern = "Primary.*\\.csv")
tbl_files <- here("data","raw","MD",csv_files) %>%
        map(~ readr::read_csv(.))
## Rename tibbles
tbl_names <- csv_files %>%
        str_extract(".*(\\(outside\\)|\\(inside\\)|\\(undefined\\))") %>% 
        str_remove("(\\ \\(outside\\)|\\ \\(inside\\)|\\ \\(undefined\\))")
names(tbl_files) <- tbl_names
## Get covariate (label, location, period, etc.) of the sensor
covariate <- csv_files %>% 
        str_extract("(\\(outside\\)|\\(inside\\)|\\(undefined\\)).*") %>% 
        str_remove_all("[\\(\\)\\-]") %>% 
        str_remove("\\.csv") %>% 
        str_split(pattern = " ") %>% 
        as_tibble(., .name_repair = "minimal") %>% 
        t(.) %>% 
        as_tibble(.) %>% 
        rename_all(~c("environment","latitude","longitude","file_type","average","start_time","end_time")) %>% 
        mutate(label = tbl_names,
               latitude = as.numeric(latitude),
               longitude = as.numeric(longitude)) %>% 
        relocate(label)
## Split covariate and data to channel A and B
covariate_A <- covariate %>% 
        filter(!str_detect(label, "( B)$"))
covariate_B <- covariate %>% 
        filter(str_detect(label, pattern = "( B)$"))

tbl_files_A <- tbl_files[!str_detect(covariate$label, "( B)$")]
tbl_files_B <- tbl_files[str_detect(covariate$label, "( B)$")]


## Select key variables from the data, rename variables, remove duplicates, 
## and add missing dates.
## Add missing date
start_time <- as_datetime("2019-01-01 00:00:00 UTC")
end_time <- as_datetime("2021-01-01 00:00:00 UTC")
interval_time <- 600 # Time interval is 10 minutes.
full_time <- as_datetime(seq(start_time, end_time, by = interval_time))
full_time <- tibble(time = full_time)

tbl_files_A_reduced <-  
        tbl_files_A[1:length(tbl_files_A)] %>% 
                map(~ select(., "created_at","UptimeMinutes","PM2.5_ATM_ug/m3",
                             "Temperature_F","Humidity_%")) %>% 
                map(~ rename(., time = created_at,
                             uptime = UptimeMinutes,
                             pm2.5 = "PM2.5_ATM_ug/m3",
                             temp = "Temperature_F",
                             humidity = "Humidity_%")) %>% 
                map(~ mutate(., time = lubridate::as_datetime(time))) %>% 
                map(~ distinct(.)) %>% # Remove duplicated rows
                map(~ right_join(., full_time, by = "time")) %>% # Add missing dates
                map(~ arrange(., time))

## Check if there is still any duplicated time point
tbl_files_A_reduced %>%
        map(~ select(., time)) %>% 
        map(~ sum(duplicated(.))) %>% 
        unlist() %>% 
        sum()

tbl_files_B_reduced <- 
        tbl_files_B[1:length(tbl_files_B)] %>% 
        map(~ select(., "created_at","UptimeMinutes","PM2.5_ATM_ug/m3")) %>% 
        map(~ rename(., time = created_at,
                     uptime = UptimeMinutes,
                     pm2.5 = "PM2.5_ATM_ug/m3")) %>% 
        map(~ mutate(., time = lubridate::as_datetime(time))) %>% 
        map(~ distinct(.)) %>% # Remove duplicated rows
        map(~ right_join(., full_time, by = "time")) %>% # Add missing dates
        map(~ arrange(., time))

## Check if there is still any duplicated time point
tbl_files_B_reduced %>%
        map(~ select(., time)) %>% 
        map(~ sum(duplicated(.))) %>% 
        unlist() %>% 
        sum()
# rm(list = c("tbl_files","tbl_files_A","tbl_files_B","csv_files","tbl_names","covariate",
            # "start_time","end_time","interval_time","full_time"))
tbl_files[1:2] %>% glimpse()
covariate_A %>% glimpse()
covariate_B %>% glimpse()
tbl_files_A_reduced[1:2] %>% glimpse()
tbl_files_B_reduced[1:2] %>% glimpse()

```

## 5. Future work

1. Do more literature review
2. Finish the data processing
3. Download EPA data and pair monitors
4. Exploratory analysis

